{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dba40669",
   "metadata": {},
   "source": [
    "# Load the Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc4c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load cleaned complaint dataset\n",
    "df = pd.read_csv('../data/filtered_complaints.csv')\n",
    "df = df[['cleaned_narrative', 'Product']]  # optionally include 'complaint_id' if you kept one\n",
    "df = df.dropna(subset=['cleaned_narrative'])  # safety check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d164b0d2",
   "metadata": {},
   "source": [
    "# Chunk the Narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b378779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 499900 text chunks.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Configure chunk size & overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Apply splitter to each row\n",
    "docs = []\n",
    "for i, row in df.iterrows():\n",
    "    chunks = text_splitter.split_text(row['cleaned_narrative'])\n",
    "    for chunk in chunks:\n",
    "        docs.append({\n",
    "            \"text\": chunk,\n",
    "            \"Product\": row['Product'],\n",
    "            \"source_id\": i  # or row['complaint_id'] if available\n",
    "        })\n",
    "\n",
    "print(f\"Generated {len(docs)} text chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3e0abd",
   "metadata": {},
   "source": [
    "# Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f9b7bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a1f9c2332549abafa967cd5bbc5aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\addis\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\addis\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb29aecd4334340a35478a75d31313e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f4991310974a0b96117a270b886d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73f86faef414df58b05fb3338223df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291d7797eea943819400faa94dcf5d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9216e99f3d7a4946b97e056389684eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2379de49ea24c0982394c11f312192a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640bd1405a1e4367b41433721f7777e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6785a38b30994348956ce18c977e448c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b7a39a2c684a658554d1503cf3c5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c778b68939489fab69497e097d03cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60fc1e0f7c8d411ba20dcd738439b7bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/15622 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Extract texts\n",
    "texts = [doc['text'] for doc in docs]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020cf662",
   "metadata": {},
   "source": [
    "#  Index with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "926b2625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store saved to 'vector_store/'\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Convert embeddings to numpy array\n",
    "embedding_dim = len(embeddings[0])\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "# Save metadata (map chunk index â†’ metadata)\n",
    "metadata = [{k: doc[k] for k in ('text', 'Product', 'source_id')} for doc in docs]\n",
    "\n",
    "# Create directory\n",
    "os.makedirs('vector_store', exist_ok=True)\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index, 'vector_store/faiss_index.index')\n",
    "\n",
    "# Save metadata\n",
    "with open('vector_store/metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(\"Vector store saved to 'vector_store/'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
